?nimbleModel
??nimbleModel
library(ggplot2)
library(data.table)
library(FKF)
library(readxl)
library(dplyr)
setwd(dirname(rstudioapi::getSourceEditorContext()$path))
data_path <- "../data/combined_Data_imm.csv"
Combined_Data <- read.csv(data_path)
# Define the parameters (phi, beta)
phi <- mean(1 - (Combined_Data$Death_Count / Combined_Data$Population))
beta <- mean(Combined_Data$Birth_Count / Combined_Data$Population)
years <- Combined_Data$Year
# Constant factor of 1 in measurement equation
Zt <- matrix(1, nrow = 1, ncol = 1)
# Initial state and variance
a0 <- as.double(Combined_Data$Population[!is.na(Combined_Data$Population)][1]) # Initial state
P0 <- matrix(1000) # Initial variance
# Constant factor of state transition matrix
Tt <- matrix(phi + beta, nrow = 1, ncol = 1)
# Observation noise (Set as 3% of the mean population count)
GGt_Default <- matrix(mean(0.03 * Combined_Data$Population), nrow = 1, ncol = 1)
########################
# Observed population
y <- Combined_Data$Population
yt <- rbind(y)
yt[is.na(yt)] <- 0 # Replace NA with 0 for Kalman filtering
dt <- matrix(as.double(Combined_Data$Immigration_Count),nrow = 1)
ct <- matrix(0) # intercepts of the transition and measurement equations
# Process noise
# Approximate with the variance of the yearly differences in population
yearly_differences <- diff(na.omit(Combined_Data$Population))
process_noise_estimate <- (var(yearly_differences))
process_noise_squared <- (sqrt(var(yearly_differences)))
HHt_Default <- matrix(process_noise_estimate)
HHt_squared <- matrix(process_noise_squared)
################################################################################################################
# Fit Kalman Filter and Plot
run_kalman_and_plot <- function(a0, P0, dt, ct, Tt, Zt, HHt, GGt, yt, years, data, plot_title) {
kf <- fkf(a0 = a0, P0 = P0, dt = dt, ct = ct, Tt = Tt, Zt = Zt, HHt = HHt, GGt = GGt, yt = yt)
log_likelihood <- kf$logLik
print(paste("Log-likelihood:", log_likelihood))
# Extract the filtered state estimates
filtered_estimates <- as.numeric(kf$att)
# Calculate standard errors (square root of diagonal elements of Ptt)
standard_errors <- list()
# Loop through each time point in the array
for (i in 1:dim(kf$Ptt)[3]) {
# Extract the standard error (square root of the variance)
se <- (kf$Ptt[1, 1, i])
# Append to the list
standard_errors[i] <- as.numeric(se)
}
standard_errors_vector <- unlist(standard_errors)
# Compute 95% Confidence Interval
ci_upper <- filtered_estimates + (1.96 * standard_errors_vector)
ci_lower <- filtered_estimates - (1.96 * standard_errors_vector)
# Convert the Kalman Filter estimates and Confidence Interval to a data frame for plotting
kf_df <- data.frame(Year = years, Estimated_Population = filtered_estimates,
CI_Upper = ci_upper, CI_Lower = ci_lower)
# Combine with the true population data
Combined_Plot_Data <- left_join(data, kf_df, by = "Year")
# Assuming `kf` is your fitted model object from the `fkf` function
log_likelihood <- kf$logLik
# Calculate k, the number of parameters estimated by your model
# This is a placeholder; you need to replace it with the actual number of parameters
k <- length(a0) + length(P0) + length(dt) + length(ct) + length(Tt) + length(Zt) + length(HHt) + length(GGt)
# Calculate AIC
AIC <- 2*k - 2*log_likelihood
print(paste("AIC:", AIC))
# Plotting with Confidence Interval
ggplot() +
geom_point(data = Combined_Plot_Data, aes(x = Year, y = Population), color = "blue", size = 2) +
geom_line(data = Combined_Plot_Data, aes(x = Year, y = Estimated_Population), color = "red") +
geom_ribbon(data = Combined_Plot_Data, aes(x = Year, ymin = ci_lower, ymax = ci_upper), alpha = 0.5, fill = "grey") +
theme_minimal()+
labs(title = plot_title) # Adding title to the plot
# Calculate and Standardize the residuals
Combined_Plot_Data$residuals <- Combined_Plot_Data$Population - Combined_Plot_Data$Estimated_Population
standard_deviation <- sd(Combined_Plot_Data$residuals, na.rm = TRUE)
Combined_Plot_Data$standardized_residuals <- Combined_Plot_Data$residuals / standard_deviation
ggplot(Combined_Plot_Data, aes(x = standardized_residuals)) +
geom_histogram(aes(y = ..density..), binwidth = .1, fill = "blue", color = "black") +
stat_function(fun = dnorm, args = list(mean = 0, sd = 1), color = "red", size = 1) +
labs(title = "Distribution of Standardized Observation Errors", x = "Standardized Residuals", y = "Density") +
theme_minimal()
}
source("optimizeKalmanParams.R")
optimized_HHt <- optimize_kalman_params(yt, Tt, Zt, a0, P0, dt, ct, HHt_Default, GGt_Default)
run_kalman_and_plot(a0, P0, dt, ct, Tt, Zt, HHt_Default, GGt_Default, yt, years, Combined_Data, 'Default process noise')
run_kalman_and_plot(a0, P0, dt, ct, Tt, Zt, HHt_Default, GGt_Default, yt, years, Combined_Data, 'Default process noise')
run_kalman_and_plot(a0, P0, dt, ct, Tt, Zt, HHt_Default, GGt_Default, yt, years, Combined_Data, 'Default process noise')
run_kalman_and_plot(a0, P0, dt, ct, Tt, Zt, optimized_HHt, GGt_Default, yt, years, Combined_Data, 'Optimized process noise')
################################################################################################################
# Fit Kalman Filter and Plot
run_kalman_and_plot <- function(a0, P0, dt, ct, Tt, Zt, HHt, GGt, yt, years, data, plot_title) {
kf <- fkf(a0 = a0, P0 = P0, dt = dt, ct = ct, Tt = Tt, Zt = Zt, HHt = HHt, GGt = GGt, yt = yt)
log_likelihood <- kf$logLik
print(paste("Log-likelihood:", log_likelihood))
# Extract the filtered state estimates
filtered_estimates <- as.numeric(kf$att)
# Calculate standard errors (square root of diagonal elements of Ptt)
standard_errors <- list()
# Loop through each time point in the array
for (i in 1:dim(kf$Ptt)[3]) {
# Extract the standard error (square root of the variance)
se <- (kf$Ptt[1, 1, i])
# Append to the list
standard_errors[i] <- as.numeric(se)
}
standard_errors_vector <- unlist(standard_errors)
# Compute 95% Confidence Interval
ci_upper <- filtered_estimates + (1.96 * standard_errors_vector)
ci_lower <- filtered_estimates - (1.96 * standard_errors_vector)
# Convert the Kalman Filter estimates and Confidence Interval to a data frame for plotting
kf_df <- data.frame(Year = years, Estimated_Population = filtered_estimates,
CI_Upper = ci_upper, CI_Lower = ci_lower)
# Combine with the true population data
Combined_Plot_Data <- left_join(data, kf_df, by = "Year")
# Assuming `kf` is your fitted model object from the `fkf` function
log_likelihood <- kf$logLik
# Calculate k, the number of parameters estimated by your model
# This is a placeholder; you need to replace it with the actual number of parameters
k <- length(a0) + length(P0) + length(dt) + length(ct) + length(Tt) + length(Zt) + length(HHt) + length(GGt)
# Calculate AIC
AIC <- 2*k - 2*log_likelihood
print(paste("AIC:", AIC))
# Plotting with Confidence Interval
ggplot() +
geom_point(data = Combined_Plot_Data, aes(x = Year, y = Population), color = "blue", size = 2) +
geom_line(data = Combined_Plot_Data, aes(x = Year, y = Estimated_Population), color = "red") +
geom_ribbon(data = Combined_Plot_Data, aes(x = Year, ymin = ci_lower, ymax = ci_upper), alpha = 0.5, fill = "grey") +
theme_minimal()+
labs(title = plot_title) # Adding title to the plot
# # Calculate and Standardize the residuals
# Combined_Plot_Data$residuals <- Combined_Plot_Data$Population - Combined_Plot_Data$Estimated_Population
#
# standard_deviation <- sd(Combined_Plot_Data$residuals, na.rm = TRUE)
# Combined_Plot_Data$standardized_residuals <- Combined_Plot_Data$residuals / standard_deviation
# ggplot(Combined_Plot_Data, aes(x = standardized_residuals)) +
#   geom_histogram(aes(y = ..density..), binwidth = .1, fill = "blue", color = "black") +
#   stat_function(fun = dnorm, args = list(mean = 0, sd = 1), color = "red", size = 1) +
#   labs(title = "Distribution of Standardized Observation Errors", x = "Standardized Residuals", y = "Density") +
#   theme_minimal()
}
run_kalman_and_plot(a0, P0, dt, ct, Tt, Zt, HHt_Default, GGt_Default, yt, years, Combined_Data, 'Default process noise')
run_kalman_and_plot(a0, P0, dt, ct, Tt, Zt, optimized_HHt, GGt_Default, yt, years, Combined_Data, 'Optimized process noise')
run_kalman_and_plot(a0, P0, dt, ct, Tt, Zt, HHt_squared, GGt_Default, yt, years, Combined_Data, 'Squared process noise')
library(ggplot2)
library(data.table)
library(dplyr)
library(nimbleSMC)
library(coda)
library(nimble)
setwd(dirname(rstudioapi::getSourceEditorContext()$path))
data_path <- "../data/combined_Data_imm.csv"
Combined_Data <- read.csv(data_path)
deviation <- 0.015 * Combined_Data$Population[length(Combined_Data$Population) - 1]
################################################################################################################
stateSpaceModelCode <- nimbleCode({
# Priors
phi ~ dunif(0, 0.1)   # phi (death rate)
beta ~ dunif(0, 0.1)  # beta (birth rate)
# Initial state
S[1] ~ dpois(lambda = N_initial) # Initial population
Y[1] ~ dlnorm(meanlog = log(S[1]), sdlog = log(sdo)) # Initial state observation
# Process model
for (t in 2:T) {
# Binomial death process
d[t] ~ dbin(size = S[t-1], prob = phi)
# Surviving population after deaths but before births
S_post_death[t] <- S[t-1] - d[t]
# Poisson birth process based on the updated surviving population
b[t] ~ dpois(lambda = beta * S_post_death[t])
# Update total surviving population including new births
S[t] ~ dpois(lambda = S_post_death[t] + b[t] + I[t])
# Observation process
Y[t] ~ dlnorm(meanlog = log(S[t]), sdlog = log(sdo))
}
})
data <- list(
Y = Combined_Data$Population,  # Observed population data
I = Combined_Data$Immigration_Count
)
constants <- list(T = nrow(Combined_Data), N_initial = Combined_Data$Population[1],
sdo = deviation)
# Initial values for the SMC
phi_initial <- mean((Combined_Data$Death_Count / Combined_Data$Population))
beta_initial <- mean(Combined_Data$Birth_Count / Combined_Data$Population)
initial_values <- list(phi = phi_initial, beta = beta_initial)
inits <- list(
phi = phi_initial,
beta = beta_initial,
N_initial = Combined_Data$Population[1] # First year's population
)
# Build the model
stateSpaceModel <- nimbleModel(stateSpaceModelCode, data = data, constants = constants, inits = inits)
# Compile the model
compiledModel <- compileNimble(stateSpaceModel)
# build bootstrap filter for state estimation
bootstrapFilter <- buildBootstrapFilter(stateSpaceModel, nodes = "S",
control = list(saveAll = TRUE, thresh = 0.7))
compiledFilter <- compileNimble(bootstrapFilter)
# Number of particles deafault 10000
parNum <- 10000
# Run bootstrap filter to estimate state sequences
compiledFilter$run(parNum)
ESS <- compiledFilter$returnESS()
# Extract equally weighted posterior samples of state variables
posteriorSamples <- as.matrix(compiledFilter$mvEWSamples)
# Generate time series estimates of the population dynamic
timeSeriesEstimates <- apply(posteriorSamples, 2, mean)
lowerCI <- apply(posteriorSamples, 2, quantile, probs = 0.025)
upperCI <- apply(posteriorSamples, 2, quantile, probs = 0.975)
# Add a Year column to timeSeriesEstimates that corresponds to the years in Population_Data
years = Combined_Data$Year  # assuming the years in Population_Data align with the indices of timeSeriesEstimates
timeSeriesEstimates_df <- data.frame(
Year = years,
Estimated_Population = timeSeriesEstimates,
Lower_CI = lowerCI,
Upper_CI = upperCI
)
ggplot(timeSeriesEstimates_df, aes(x = Year, y = Estimated_Population)) +
geom_ribbon(aes(ymin = Lower_CI, ymax = Upper_CI), alpha = 0.3, col = "grey", size = 2) +  # Confidence interval band
geom_line(col = "red", size = 0.5) +
geom_point(data = Combined_Data, aes(x = Year, y = Population), col = "blue", pch = 16) +
xlab("Year") + ylab("Population") +
ggtitle("SMC - Population Estimates vs Actual with 95% CI") +
theme_minimal()
uniqueParticleCounts <- apply(posteriorSamples, 2, function(x) length(unique(x)))
plot(ESS)
plot(uniqueParticleCounts)
plot(uniqueParticleCounts, 'Unique Particles over Iterations')
plot(uniqueParticleCounts, main = 'Unique Particles over Iterations', ylab = 'count', xlab = 'Index')
plot(uniqueParticleCounts, main = 'Unique Particles over Iterations', ylab = 'Count', xlab = 'Index')
hist(posteriorSamples[,183], main = 'Posterior Distribution of 2021 Population Estimate', xlab = 'Count')
###################################
## create MCMC specification for the state space model
stateSpaceMCMCconf <- configureMCMC(stateSpaceModel, nodes = NULL)
## add a block pMCMC sampler for a, b, sigPN, and sigOE
stateSpaceMCMCconf$addSampler(target = c('phi', 'beta'),
type = 'RW_PF_block', control = list(latents = 'S'))
## build and compile pMCMC sampler
stateSpaceMCMC <- buildMCMC(stateSpaceMCMCconf)
compiledList <- compileNimble(stateSpaceModel, stateSpaceMCMC, resetFunctions = TRUE)
compiledList$stateSpaceMCMC$run(5000)
par(mfrow = c(2,1))
posteriorSamps <- as.mcmc(as.matrix(compiledList$stateSpaceMCMC$mvSamples))
traceplot(posteriorSamps[,'phi'], ylab = 'phi')
traceplot(posteriorSamps[,'beta'], ylab = 'beta')
hist(posteriorSamps[,'phi'])
hist(posteriorSamps[,'beta'])
par(mfrow = c(1,1))
library(tidyverse)
library(dplyr)
library(ggplot2)
# Read the data
setwd(dirname(rstudioapi::getSourceEditorContext()$path))
data_path <- "../data/combined_Data_imm.csv"
data <- read.csv(data_path)
bayesian_particle_filter_adjusted <- function(data, num_particles = 10000, birth_rate_prior = c(0, 0.1), death_rate_prior = c(0, 0.1), thresh = 0.8) {
set.seed(4599)
overall_log_likelihood <- 0
birth_rate_particles  <- runif(num_particles, birth_rate_prior[1], birth_rate_prior[2])
death_rate_particles <- runif(num_particles, death_rate_prior[1], death_rate_prior[2])
weights <- rep(1/num_particles, num_particles)
estimated_parameters_and_population <- data.frame()
estimated_birth_rate <-  data$Birth_rate[1]
estimated_death_rate <- data$Death_rate[1]
final_birth_rate_particles <- NULL
final_death_rate_particles <- NULL
final_estimated_population <- NULL
for (year in 2:nrow(data)) {
prev_population <- data[year - 1, 'Population']
observed_population <- data[year, 'Population']
sdo <- sqrt(0.03 * prev_population) # Standard Deviation of Observation each Year
# Sample around the estimates from the previous year
birth_rate_particles <- pmax(rnorm(num_particles, mean = estimated_birth_rate, sd = 0.01), 0)
death_rate_particles <- pmax(rnorm(num_particles, mean = estimated_death_rate, sd = 0.01), 0)
# Simulate process model with changes:x
died <- rbinom(num_particles, prev_population, death_rate_particles)  # Binomial death
survived <- prev_population - died                                   # Survivors
births <- rpois(num_particles, lambda = survived * birth_rate_particles) # Poisson birth
estimated_population <- survived + births + data[year, "Immigration_Count"]
# Add normal variation for process error
estimated_population <- estimated_population
# Update weights
unnormalized_weights <- weights * dlnorm(observed_population, meanlog = log(estimated_population), sdlog = log((sdo)), log = FALSE)
overall_log_likelihood <- overall_log_likelihood + log(sum((unnormalized_weights))/num_particles)
weights <- unnormalized_weights / sum(unnormalized_weights)
# Resample particles
ess <- 1 / (sum(weights^2))
if (ess < num_particles * thresh) { # Resample if ESS below threshold
indices <- sample(1:num_particles, size = num_particles, replace = TRUE, prob = weights)
birth_rate_particles <- birth_rate_particles[indices]
death_rate_particles <- death_rate_particles[indices]
weights <- rep(1/num_particles, num_particles) # Reset weights after resampling
}
# Estimate parameters
estimated_birth_rate <- mean(birth_rate_particles)
estimated_death_rate <- mean(death_rate_particles)
# Calculate Confidence Intervals
lower_CI <- quantile(estimated_population, probs = 0.025)
upper_CI <- quantile(estimated_population, probs = 0.975)
# Print unique particle counts
cat("Year:", data[year, 'Year'], "- Unique Particles:", length(unique(estimated_population)))
estimated_parameters_and_population <- rbind(estimated_parameters_and_population, data.frame(
Year = data[year, 'Year'],
Estimated_birth_Rate = estimated_birth_rate,
Estimated_Death_Rate = estimated_death_rate,
Observed_Population = observed_population,
Estimated_Population = mean(estimated_population),
Lower_CI = lower_CI,
Upper_CI = upper_CI,
ESS = ess,
Likelihood = overall_log_likelihood
))
}
# Update final particles at each iteration
final_birth_rate_particles <- birth_rate_particles
final_death_rate_particles <- death_rate_particles
final_estimated_population <- estimated_population
return(list(
Estimated_Parameters = estimated_parameters_and_population,
Final_Birth_Rate_Particles = final_birth_rate_particles,
Final_Death_Rate_Particles = final_death_rate_particles,
Final_Estimated_Population = final_estimated_population
))}
# Run the Boostrap Model
results <- bayesian_particle_filter_adjusted(data)
final_birth_rates <- results$Final_Birth_Rate_Particles
final_death_rates <- results$Final_Death_Rate_Particles
final_estimated_populations <- results$Final_Estimated_Population
par(mfrow=c(1,3))
hist(final_estimated_populations, main = 'Estimated Populations')
abline(v = tail(data$Population, 1), col = "red", lwd = 2)
hist(final_birth_rates, main = 'Estimated Birth rate')
abline(v = tail(data$Birth_rate, 1), col = "red", lwd = 2)
hist(final_death_rates, main = 'Estimated Death rate')
abline(v = tail(data$Death_rate, 1), col = "red", lwd = 2)
adjusted_bayesian_estimated_population_df <- results$Estimated_Parameters
cat("Log Likelihood:", tail(adjusted_bayesian_estimated_population_df$Likelihood,1), "\n")
# Plot results
ggplot(adjusted_bayesian_estimated_population_df, aes(x = Year)) +
geom_line(aes(y = Observed_Population, color = "Actual Population")) +
geom_line(aes(y = Estimated_Population, color = "Estimated Population")) +
geom_ribbon(aes(ymin = Lower_CI, ymax = Upper_CI), alpha = 0.3, col = "grey") +
xlab('Year') +
ylab('Population') +
ggtitle('Particle Filtering Hard-Coded') +
scale_color_manual(values = c('Actual Population' = 'blue', 'Estimated Population' = 'red')) +
theme_minimal()
ggplot(adjusted_bayesian_estimated_population_df, aes(x = Year)) +
geom_line(aes(y = Estimated_Death_Rate, color = "Estimated Death Rate")) +
geom_line(aes(y = Estimated_birth_Rate, color = "Estimated Birth Rate")) +
xlab('Year') +
ylab('Rate %') +
ggtitle('Estimated Birth & Death Rate') +
scale_color_manual(values = c('Estimated Death Rate' = 'blue', 'Estimated Birth Rate' = 'red')) +
theme_minimal()
residuals <- adjusted_bayesian_estimated_population_df$Observed_Population - adjusted_bayesian_estimated_population_df$Estimated_Population
adjusted_bayesian_estimated_population_df$Residuals <- residuals
plot(adjusted_bayesian_estimated_population_df$Year, adjusted_bayesian_estimated_population_df$Residuals,  ylab = "One-Step Ahead Residuals", xlab = "Year")
par(mfrow=c(1,1))
residuals <- adjusted_bayesian_estimated_population_df$Observed_Population - adjusted_bayesian_estimated_population_df$Estimated_Population
adjusted_bayesian_estimated_population_df$Residuals <- residuals
plot(adjusted_bayesian_estimated_population_df$Year, adjusted_bayesian_estimated_population_df$Residuals,  ylab = "One-Step Ahead Residuals", xlab = "Year")
plot(adjusted_bayesian_estimated_population_df$Year, adjusted_bayesian_estimated_population_df$Residuals,  ylab = "Count", xlab = "Year"
main = "One-Step-Ahead Predictive Residuals")
plot(adjusted_bayesian_estimated_population_df$Year, adjusted_bayesian_estimated_population_df$Residuals,  ylab = "Count", xlab = "Year",
main = "One-Step-Ahead Predictive Residuals")
plot(adjusted_bayesian_estimated_population_df$Year, adjusted_bayesian_estimated_population_df$Residuals,  ylab = "Count", xlab = "Year",
type = 'l', main = "One-Step-Ahead Predictive Residuals")
